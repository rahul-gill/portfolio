---
title: Synchronization
index: 3
---


## Critical section problem
- critical section: piece of code that accesses shared resources
- race condition: if multiple threads enter the critical section at the same time
- indeterminant program: have some race conditions existing in it
- mutual exclusion: guarantees that only single thread at a time enters the critical section
- atomic operations: all/none group of instructions: either all sub-operations happen together or non of them happen
- group of actions as a single atomic action is also called transaction in database systems
- in a nonpreemptive kernel, a process running in kernel mode cannot be preempted. The process will run until it exits, blocks or voluntarilty yield the cpu.
- nonpreemptive kernel won't cause race conditions but preemptive kernel is more responsive and less risk of kernel process running arbitrarily long

<br/>

- the data structure used to provide mutual exclusion called mutex
```c
lock_t mutex;
init(&mutex);
lock(&mutex);
//critical section
unlocK(&mutex);
```
- once a process locks the mutex, it has locked on the critical section and no other process can enter the critical section; they'll have to wait until this process exits the critical section
- enabling and disabling interrupts for locking and unlocking mutexes is not good solution because
    - we have to trust the process
    - doesn't work with multi-processor systems
    - inefficient(masking/unmasking interrupts slow)
    - missing interrupts
- **peterson's solution**
```c
int flag[2], turn;
void init(lock_t *mutex){
	flag[0] = flag[1] = 0;// 1->thread wants to grab lock
	turn = 0;// whose turn? (thread 0 or 1?)
}
void lock(lock_t *mutex){
	flag[self] = 1;// self: thread ID of caller
	turn = 1 - self;// make it other threadâ€™s turn
	while ((flag[1-self] == 1) && (turn == 1 - self))
		;// spin-wait

}
void unlock(lock_t motex){
	flag[self] = 0;// simply undo your intent
}
```
- won't work on modern architectures

<br/>

- lock software based solution(bad because of incorrectness and bad performance)
```c
typedef struct __lock_t{ int flag; } lock_t;
void init(lock_t *mutex){
    mutex->flag = 0;//0 means lock available
}
void lock(lock_t *mutex){
    while(mutex->flag==1)
        ;//spin wait
    mutex->flag=1;
}
void unlock(lock_t *mutex){
    mutex->flag=0;
}
```
## Locks with hardware support
- some atomic instructions provided by cpu
### **test and set**(atomic exchange) cpu instructions
- use of instructions like xchg(x86) and ldstub(sparc)
- pseudo code:
```c
int test_and_set(int *old_ptr, int new){
   int old = *old_ptr;
   *old_ptr = new;
   return old;
}
```
- with it, the previous lock code will be modified as
```c
while(test_and_set(&mutex->flag, 1) == 1) ;
```
- it doesn't provide any fairness guarantees
- bad performance of single core(imagine N processes waiting for the lock, only $1/nth$ of total cpu time will do the main work
- on multi core reasonably well performance if number of threads roughly equals to number of cores
###  **compare and swap**
- pseudo code
```c
int compare_and_swap(int *ptr, expected, new) {
    int actual = *ptr;
    if (actual == expected) *ptr = new;
    return actual;
}
```
- with it, the previous lock code will be modified as
```c
while(compare_and_swap(&mutex->flag ,0 , 1) == 1) ;
```
###  **load linked and store conditional**
- pseudo code
 ```c
 int load_linked(int *ptr){ return *ptr }
 int store_conditional(int *ptr, int expected, int new) {
     if (/*no one has updated *ptr since the LoadLinked to this address*/) {
         *ptr = value;
         return 1;//success
     }
     else return 0;
 }
 ```
- with it, the previous lock code will be modified as
 ```c
 while (LoadLinked(&lock->flag)||
        !StoreConditional(&lock->flag, 1))
      ;//spin
 ```
###  **fetch and add**
- pseudo code
 ```c
 int fetch_and_add(int *ptr){
     int old = *ptr;
     *ptr = old + 1;
     return old;
 }
 ```
- ticket lock based on it
 ```c
 typedef struct __lock_t {
      int now_serving;
      int next_ticket;
  } lock_t;
 void lock_init(lock_t *lock) {
     lock->now_serving = lock->next_ticket = 0;
 }
 void lock(lock_t *lock){
     int = my_ticket;
     fetch_and_add(lock->next_ticket);
     while (lock->now_serving != my_ticket)
         ;
  }
  void unlock (lock_t *lock)  { lock->now_serving++; }
 ```
- this lock is fair. Every process will get the chance to run critical section after all processes with tickets less that its finish

## Better locks with os support
- spinning is also called busy waiting
- getting ahead of spin locks: yield(give up cpu) but still context switches happen and thats pretty heavy
```c
void init(lock_t *lock) {
lock->flag = 0;
}
void lock(lock_t *lock) {
while (test_and_set(&lock->flag, 1) == 1)
	yield(); // give up the CPU
}
void unlock(lock_t *lock) {
lock->flag = 0;
}
```

- using park, unpark(in solaris) and **queue**
    -  sycall `park` makes the process sleep
    -  sycall `unpark` makes the process wake up from the instruction just next to the `park` syscall
```c
typedef struct __lock_t {
	int flag;
	int guard;
	queue_t *q;
} lock_t;
void lock_init(lock_t *m) {
	m->flag = 0;
	m->guard = 0;
	queue_init(m->q);
}
void lock(lock_t *m) {
	while (test_and_set(&m->guard, 1) == 1)
		; //acquire guard lock by spinning
	if (m->flag == 0) {
		m->flag = 1; // lock is acquired
		m->guard = 0;
	} else {
		queue_add(m->q, gettid());
		m->guard = 0;
		park();
	}
}
void unlock(lock_t *m) {
	while (test_and_set(&m->guard, 1) == 1)
		; //acquire guard lock by spinning
	if (queue_empty(m->q))
		m->flag = 0; // let go of lock; no one wants it
	else
		unpark(queue_remove(m->q)); // hold lock (for next thread!)
	m->guard = 0;
}
```

- important points about it
    - if the release of the guard lock came after the  `park` problems will occur because the process will sleep before releasing guard and the process currently in the critical section won't be able to do the work in unlock subroutine.
    - flag is not set to 0 in unlock subroutine because calling unpark will resume the parked process from just the next instruction to call to park. The parked thread will straight go into the critical section after being unparked
    - If a thread calling lock subroutine finds that another thread has acquired the lock and it has executed the `guard= 0; ` line. If now a context switch to the thread currently in critical section happen and that thread calls unlock: wakeup/waiting race
    - to fix this solaris ha s `setpark` syscall. Calling it indicates that thread is about to sleep. If it happens to be interrupted and another thread calls unpark before park is actually called, the subsequent park returns immediately instead of sleeping.
    - we just have to add `setpark();` line between `queue _add` and `lock->guard=0;`
    - a different solution could be to let kernel handle the `guard` variable.

<br/>

- linux has **futex** for these things
- futex consists of a kernelspace wait queue that is attached to an atomic integer in userspace. Multiple processes or thread operate on the integer entirely in userspace (using atomic operations), and only resort to relatively expensive syscalls to request operations on the wait queue (for example to wake up waiting processes, or to put the current process on the wait queue).
- call to futex wait(address,expected) puts the calling thread to sleep, assuming the value at address is equal to expected. If it is not equal, the call returns immediately. The call to the routine futex wake(address)wakes one thread that is waiting on the queue
- code snippet from lowlevellock.h in the nptl library (part of the gnu libc library)
```c
void mutex_lock (int *mutex) {
	int v;
	/* Bit 31 was clear, we got the mutex (this is the fastpath)*/
	if (atomic_bit_test_set (mutex, 31) == 0)
		return;
	atomic_increment (mutex);
	while (1) {
		if (atomic_bit_test_set (mutex, 31) == 0) {
		atomic_decrement (mutex);
		return;
	}
		/* We have to wait now. First make sure the futex value
		we are monitoring is truly negative (i.e. locked). */
		v = *mutex;
		if (v >= 0) continue;
		futex_wait (mutex, v);
	}
}
void mutex_unlock (int *mutex) {
	/* Adding 0x80000000 to the counter results in 0 if and only if there are not other interested threads */
/* Its same as zeroing ms bit*/
	if (atomic_add_zero (mutex, 0x80000000))
		return;
	/* There are other threads waiting for this mutex, wake one of them up. */
	futex_wake (mutex);
}
```
- two phase locking: the lock spin once(or k times) to check if lock got free
- [Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask](https://dl.acm.org/doi/10.1145/2517349.2522714)
